pluto_tile_parallel:
0.2525292570


default tvm:
Execution time of this operator: max:2.3649904927 s   median:2.3432134147 s   min:2.3172877989 s

ansor:
Execution time of this operator: max:0.0042784552 s   median:0.0031172393 s   min:0.0030021192 s

ppcg cuda:
0.003899392


gpu ansor:Execution time of this operator: max:0.0004822514 s   median:0.0004656083 s   min:0.0004441458 s
Equivalent python schedule:
compute_i, compute_j, compute_k = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i_o_i, compute_i_i = s[compute].split(compute_i, factor=2)
compute_i_o_o_i, compute_i_o_i = s[compute].split(compute_i_o_i, factor=8)
compute_i_o_o_o_i, compute_i_o_o_i = s[compute].split(compute_i_o_o_i, factor=8)
compute_i_o_o_o_o, compute_i_o_o_o_i = s[compute].split(compute_i_o_o_o_i, factor=1)
compute_j_o_i, compute_j_i = s[compute].split(compute_j, factor=4)
compute_j_o_o_i, compute_j_o_i = s[compute].split(compute_j_o_i, factor=1)
compute_j_o_o_o_i, compute_j_o_o_i = s[compute].split(compute_j_o_o_i, factor=16)
compute_j_o_o_o_o, compute_j_o_o_o_i = s[compute].split(compute_j_o_o_o_i, factor=1)
compute_k_o_i, compute_k_i = s[compute].split(compute_k, factor=2)
compute_k_o_o, compute_k_o_i = s[compute].split(compute_k_o_i, factor=4)
s[compute].reorder(compute_i_o_o_o_o, compute_j_o_o_o_o, compute_i_o_o_o_i, compute_j_o_o_o_i, compute_i_o_o_i, compute_j_o_o_i, compute_k_o_o, compute_k_o_i, compute_i_o_i, compute_j_o_i, compute_k_i, compute_i_i, compute_j_i)
compute_i_o_i, compute_i_i = s[compute].split(compute_i, factor=16)
compute_i_o_o_i, compute_i_o_i = s[compute].split(compute_i_o_i, factor=8)
compute_i_o_o_o, compute_i_o_o_i = s[compute].split(compute_i_o_o_i, factor=1)
compute_j_o_i, compute_j_i = s[compute].split(compute_j, factor=4)
compute_j_o_o_i, compute_j_o_i = s[compute].split(compute_j_o_i, factor=16)
compute_j_o_o_o, compute_j_o_o_i = s[compute].split(compute_j_o_o_i, factor=1)
s[compute].reorder(compute_i_o_o_o, compute_j_o_o_o, compute_i_o_o_i, compute_j_o_o_i, compute_i_o_i, compute_j_o_i, compute_i_i, compute_j_i)
s[compute].compute_at(s[compute], compute_j_o_i)
B_shared = s.cache_read(B, "shared", [compute])
B_shared_ax0, B_shared_ax1 = tuple(B_shared.op.axis)
s[B_shared].compute_at(s[compute], compute_k_o_o)
A_shared = s.cache_read(A, "shared", [compute])
A_shared_ax0, A_shared_ax1 = tuple(A_shared.op.axis)
s[A_shared].compute_at(s[compute], compute_k_o_o)
compute_i_o_o_o_j_o_o_o_fused = s[compute].fuse(compute_i_o_o_o, compute_j_o_o_o)
s[compute].bind(compute_i_o_o_o_j_o_o_o_fused, te.thread_axis("blockIdx.x"))
compute_i_o_o_i_j_o_o_i_fused = s[compute].fuse(compute_i_o_o_i, compute_j_o_o_i)
s[compute].bind(compute_i_o_o_i_j_o_o_i_fused, te.thread_axis("vthread"))
compute_i_o_i_j_o_i_fused = s[compute].fuse(compute_i_o_i, compute_j_o_i)
s[compute].bind(compute_i_o_i_j_o_i_fused, te.thread_axis("threadIdx.x"))
B_shared_ax0_ax1_fused = s[B_shared].fuse(B_shared_ax0, B_shared_ax1)
B_shared_ax0_ax1_fused_o, B_shared_ax0_ax1_fused_i = s[B_shared].split(B_shared_ax0_ax1_fused, factor=4)
s[B_shared].vectorize(B_shared_ax0_ax1_fused_i)
B_shared_ax0_ax1_fused_o_o, B_shared_ax0_ax1_fused_o_i = s[B_shared].split(B_shared_ax0_ax1_fused_o, factor=128)
s[B_shared].bind(B_shared_ax0_ax1_fused_o_i, te.thread_axis("threadIdx.x"))
A_shared_ax0_ax1_fused = s[A_shared].fuse(A_shared_ax0, A_shared_ax1)
A_shared_ax0_ax1_fused_o, A_shared_ax0_ax1_fused_i = s[A_shared].split(A_shared_ax0_ax1_fused, factor=2)
s[A_shared].vectorize(A_shared_ax0_ax1_fused_i)
A_shared_ax0_ax1_fused_o_o, A_shared_ax0_ax1_fused_o_i = s[A_shared].split(A_shared_ax0_ax1_fused_o, factor=128)
s[A_shared].bind(A_shared_ax0_ax1_fused_o_i, te.thread_axis("threadIdx.x"))
s[compute].pragma(compute_i_o_o_o_o, "auto_unroll_max_step", 1024)
s[compute].pragma(compute_i_o_o_o_o, "unroll_explicit", True)