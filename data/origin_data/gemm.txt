pluto_tile_parallel:
0.6044747700


default tvm:
Execution time of this operator: max:1.3057001008 s   median:1.2959689423 s   min:1.2950700992 s

default ansor:
Execution time of this operator: max:3779332.302 ns   median:3711960.992 ns   min:3356732.527 ns
Equivalent python schedule:
compute_i, compute_j, compute_k = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
s[compute].compute_inline()
compute_i_o_i, compute_i_i = s[compute].split(compute_i, factor=4)
compute_i_o_o_i, compute_i_o_i = s[compute].split(compute_i_o_i, factor=16)
compute_i_o_o_o, compute_i_o_o_i = s[compute].split(compute_i_o_o_i, factor=4)
compute_j_o_i, compute_j_i = s[compute].split(compute_j, factor=16)
compute_j_o_o_i, compute_j_o_i = s[compute].split(compute_j_o_i, factor=1)
compute_j_o_o_o, compute_j_o_o_i = s[compute].split(compute_j_o_o_i, factor=16)
compute_k_o, compute_k_i = s[compute].split(compute_k, factor=32)
s[compute].reorder(compute_i_o_o_o, compute_j_o_o_o, compute_i_o_o_i, compute_j_o_o_i, compute_k_o, compute_i_o_i, compute_j_o_i, compute_k_i, compute_i_i, compute_j_i)
compute_i_o_i, compute_i_i = s[compute].split(compute_i, factor=64)
compute_i_o_o, compute_i_o_i = s[compute].split(compute_i_o_i, factor=4)
compute_j_o_i, compute_j_i = s[compute].split(compute_j, factor=16)
compute_j_o_o, compute_j_o_i = s[compute].split(compute_j_o_i, factor=16)
s[compute].reorder(compute_i_o_o, compute_j_o_o, compute_i_o_i, compute_j_o_i, compute_i_i, compute_j_i)
s[compute].compute_at(s[compute], compute_j_o_i)
compute_i_o_o_j_o_o_fused_i_o_i_fused_j_o_i_fused = s[compute].fuse(compute_i_o_o, compute_j_o_o, compute_i_o_i, compute_j_o_i)
s[compute].parallel(compute_i_o_o_j_o_o_fused_i_o_i_fused_j_o_i_fused)
s[compute].pragma(compute_i_o_o_o, "auto_unroll_max_step", 512)
s[compute].pragma(compute_i_o_o_o, "unroll_explicit", True)
s[compute].vectorize(compute_j_i)

ppcg cuda:
0.0028824639




gpu ansor:Execution time of this operator: max:0.000569596634 s   median:0.000560372605 s   min:0.000553357337 s
Equivalent python schedule:
compute_i, compute_j, compute_k = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
s[compute].compute_inline()
compute_i_o_i, compute_i_i = s[compute].split(compute_i, factor=1)
compute_i_o_o_i, compute_i_o_i = s[compute].split(compute_i_o_i, factor=4)
compute_i_o_o_o_i, compute_i_o_o_i = s[compute].split(compute_i_o_o_i, factor=16)
compute_i_o_o_o_o, compute_i_o_o_o_i = s[compute].split(compute_i_o_o_o_i, factor=2)
compute_j_o_i, compute_j_i = s[compute].split(compute_j, factor=1)
compute_j_o_o_i, compute_j_o_i = s[compute].split(compute_j_o_i, factor=2)
compute_j_o_o_o_i, compute_j_o_o_i = s[compute].split(compute_j_o_o_i, factor=16)
compute_j_o_o_o_o, compute_j_o_o_o_i = s[compute].split(compute_j_o_o_o_i, factor=2)
compute_k_o_i, compute_k_i = s[compute].split(compute_k, factor=1)
compute_k_o_o, compute_k_o_i = s[compute].split(compute_k_o_i, factor=32)
s[compute].reorder(compute_i_o_o_o_o, compute_j_o_o_o_o, compute_i_o_o_o_i, compute_j_o_o_o_i, compute_i_o_o_i, compute_j_o_o_i, compute_k_o_o, compute_k_o_i, compute_i_o_i, compute_j_o_i, compute_k_i, compute_i_i, compute_j_i)
compute_i_o_i, compute_i_i = s[compute].split(compute_i, factor=4)
compute_i_o_o_i, compute_i_o_i = s[compute].split(compute_i_o_i, factor=16)
compute_i_o_o_o, compute_i_o_o_i = s[compute].split(compute_i_o_o_i, factor=2)
compute_j_o_i, compute_j_i = s[compute].split(compute_j, factor=2)
compute_j_o_o_i, compute_j_o_i = s[compute].split(compute_j_o_i, factor=16)
compute_j_o_o_o, compute_j_o_o_i = s[compute].split(compute_j_o_o_i, factor=2)
s[compute].reorder(compute_i_o_o_o, compute_j_o_o_o, compute_i_o_o_i, compute_j_o_o_i, compute_i_o_i, compute_j_o_i, compute_i_i, compute_j_i)
s[compute].compute_at(s[compute], compute_j_o_i)
B_shared = s.cache_read(B, "shared", [compute])
B_shared_ax0, B_shared_ax1 = tuple(B_shared.op.axis)
s[B_shared].compute_at(s[compute], compute_k_o_o)
A_shared = s.cache_read(A, "shared", [compute])
A_shared_ax0, A_shared_ax1 = tuple(A_shared.op.axis)
s[A_shared].compute_at(s[compute], compute_k_o_o)
compute_i_o_o_o_j_o_o_o_fused = s[compute].fuse(compute_i_o_o_o, compute_j_o_o_o)
s[compute].bind(compute_i_o_o_o_j_o_o_o_fused, te.thread_axis("blockIdx.x"))
compute_i_o_o_i_j_o_o_i_fused = s[compute].fuse(compute_i_o_o_i, compute_j_o_o_i)
s[compute].bind(compute_i_o_o_i_j_o_o_i_fused, te.thread_axis("vthread"))
compute_i_o_i_j_o_i_fused = s[compute].fuse(compute_i_o_i, compute_j_o_i)
s[compute].bind(compute_i_o_i_j_o_i_fused, te.thread_axis("threadIdx.x"))
B_shared_ax0_ax1_fused = s[B_shared].fuse(B_shared_ax0, B_shared_ax1)
B_shared_ax0_ax1_fused_o, B_shared_ax0_ax1_fused_i = s[B_shared].split(B_shared_ax0_ax1_fused, factor=2)
s[B_shared].vectorize(B_shared_ax0_ax1_fused_i)
B_shared_ax0_ax1_fused_o_o, B_shared_ax0_ax1_fused_o_i = s[B_shared].split(B_shared_ax0_ax1_fused_o, factor=256)
s[B_shared].bind(B_shared_ax0_ax1_fused_o_i, te.thread_axis("threadIdx.x"))
A_shared_ax0_ax1_fused = s[A_shared].fuse(A_shared_ax0, A_shared_ax1)
A_shared_ax0_ax1_fused_o, A_shared_ax0_ax1_fused_i = s[A_shared].split(A_shared_ax0_ax1_fused, factor=2)
s[A_shared].vectorize(A_shared_ax0_ax1_fused_i)
A_shared_ax0_ax1_fused_o_o, A_shared_ax0_ax1_fused_o_i = s[A_shared].split(A_shared_ax0_ax1_fused_o, factor=256)
s[A_shared].bind(A_shared_ax0_ax1_fused_o_i, te.thread_axis("threadIdx.x"))
s[compute].pragma(compute_i_o_o_o_o, "auto_unroll_max_step", 1024)
s[compute].pragma(compute_i_o_o_o_o, "unroll_explicit", True)