pluto_tile_parallel:
0.6298039150


tvm default:Execution time of this operator: max:0.8787099637 s   median:0.8787099637 s   min:0.8787099637 s

padding default:Execution time of this operator: max:0.4508870087 ns   median:0.4435270299 ns   min:0.4424751031 ns

ansor:Execution time of this operator: max:0.0154131096 s   median:0.0152551052 s   min:0.0141405582 s

padding ansor Execution time of this operator: max:0.0133317999 s   median:0.0130582450 s   min:0.0128949558 s

ppcg cuda:
0.0109384321


gpu ansor:
Execution time of this operator: max:0.0040315099 s   median:0.0040232151 s   min:0.0039982717 s
Equivalent python schedule:
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j, compute_k = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_i, compute_j = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
compute_j_o, compute_j_i = s[compute].split(compute_j, factor=64)
s[compute].bind(compute_j_i, te.thread_axis("threadIdx.x"))
compute_k_o, compute_k_i = s[compute].split(compute_k, factor=64)
s[compute].bind(compute_k_i, te.thread_axis("threadIdx.x"))
s[compute].compute_at(s[compute], compute_j_o)
s[compute].compute_inline()
compute_i_j_o_fused = s[compute].fuse(compute_i, compute_j_o)
s[compute].bind(compute_i_j_o_fused, te.thread_axis("blockIdx.x"))
s[compute].pragma(compute_i, "auto_unroll_max_step", 1024)
s[compute].pragma(compute_i, "unroll_explicit", True)

padding  ansor_gpu:0.002884425